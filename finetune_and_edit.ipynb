{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "finetune-and-edit.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/16A0/til/blob/master/finetune_and_edit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9RImQ7DV_8y"
      },
      "source": [
        "# Latent Composition Finetune and Edit\n",
        "\n",
        "For real images, finetune the encoder towards a specific image for better reconstruction. Further composition can be done in real time.\n",
        "\n",
        "Related Colab Notebooks:\n",
        "- [Interactive Masking Demo]( https://colab.research.google.com/drive/1p-L2dPMaqMyr56TYoYmBJhoyIyBJ7lzH?usp=sharing): Demonstrates using a masked encoder to investigate image priors in GANs.\n",
        "- [Interactive Composition Demo](https://colab.research.google.com/drive/1j7Bz9vdVnxzOgokawA39hCJZLTmVDq6_?usp=sharing): Interface to compose multiple images using masked encoder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4acfzEryWPHN"
      },
      "source": [
        "## Download code, models, and set up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpHfeuJ2UUyo"
      },
      "source": [
        "! git clone https://github.com/chail/latent-composition.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7rkPtzqWyA5"
      },
      "source": [
        "import os\n",
        "os.chdir('latent-composition')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7h3lcrR5cQh5"
      },
      "source": [
        "# download pretrained models and unzip them (this may take a few minutes)\n",
        "print(\"Downloading pretrained models...\")\n",
        "! gdown --id 1vSEH2XMIG1XzQl3JLZwUKm_kdomelUqm # try download from gdrive first\n",
        "if not os.path.isfile('pretrained_models.zip'):\n",
        "  # try download from csail (will be slower than gdrive)\n",
        "  print(\"Gdrive download failed, trying backup copy...\")\n",
        "  ! wget http://people.csail.mit.edu/lrchai/projects/latent-composition/pretrained_models.zip\n",
        "\n",
        "assert(os.path.isfile('pretrained_models.zip')),\"pretrained_models.zip not found!\"\n",
        "print(\"Finished downloading. Unpacking models...\")\n",
        "! unzip pretrained_models.zip\n",
        "print(\"Done!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNp2HAmrdNaG"
      },
      "source": [
        "# required for stylegan models\n",
        "! pip install ninja"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRDFgZRyewWf"
      },
      "source": [
        " os.environ['TORCH_EXTENSIONS_DIR'] = '/tmp/torch_cpp/' # needed for stylegan to run"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzFRyqDfkYaY"
      },
      "source": [
        "# additional requirements for finetuning\n",
        "! pip install lpips\n",
        "# face landmarks model\n",
        "! mkdir -p resources/dlib\n",
        "! wget http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n",
        "! mv shape_predictor_68_face_landmarks.dat.bz2 resources/dlib\n",
        "! bunzip2 resources/dlib/shape_predictor_68_face_landmarks.dat.bz2\n",
        "# identity loss model from pixel2style2pixel\n",
        "! gdown --id 1KW7bjndL3QG3sxBbZxreGHigcCCpsDgn # pretrained model\n",
        "! mkdir -p resources/psp\n",
        "! mv model_ir_se50.pth resources/psp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XiV386Yaldwz"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from utils import show, renormalize, masking\n",
        "from utils import util, imutil, pbar, losses, inversions\n",
        "from networks import networks\n",
        "from PIL import Image\n",
        "import os\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAChjrJBSpJD"
      },
      "source": [
        "assert(torch.cuda.is_available()) # check cuda is available"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYA1NOa8U0It"
      },
      "source": [
        "# Load Networks\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_hJD3JyUyby"
      },
      "source": [
        "def load_nets():\n",
        "    # bonus: change this to use stylegan encoder finetuned\n",
        "    # on real images + identity loss (not used in paper)\n",
        "    # nets = networks.define_nets('stylegan', 'ffhq', ckpt_path='pretrained_models/sgan_encoders/ffhq_reals_RGBM/netE_epoch_best.pth')\n",
        "    \n",
        "    # stylegan trained on gsamples + identity loss\n",
        "    nets = networks.define_nets('stylegan', 'ffhq')\n",
        "\n",
        "    return nets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2vfmJ8_l3T5"
      },
      "source": [
        "# Setup input photo:\n",
        "You can choose to upload a photo, the code will first try to align it by facial landmarks (please make sure the face is sufficiently visible). Otherwise, you can cancel the upload to use the default image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEQdlx2vmA7h"
      },
      "source": [
        "from google.colab import files\n",
        "from utils import face_crop\n",
        "import cv2\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "if uploaded:\n",
        "  target_image_name = list(uploaded.keys())[0]\n",
        "  target_image = np.array(Image.open(target_image_name).convert('RGB'))\n",
        "  target_image, lm = face_crop.celebahq_crop(target_image)\n",
        "  os.remove(target_image_name)\n",
        "  target_image.save(target_image_name)\n",
        "  im_path = target_image_name\n",
        "else:\n",
        "  im_path = 'img/torralba_cropped.png'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpbDbfcvlrRa"
      },
      "source": [
        "outdim=1024 # for faces\n",
        "# outdim = 256 # for churches\n",
        "\n",
        "transform = transforms.Compose([\n",
        "                transforms.Resize(outdim),\n",
        "                transforms.CenterCrop(outdim),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "            ])    \n",
        "source_im = transform(Image.open(im_path).convert('RGB'))[None].cuda()\n",
        "show(['Source Image', renormalize.as_image(source_im[0]).resize((256, 256), Image.LANCZOS)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQCZMZ2oVA2e"
      },
      "source": [
        "\n",
        "\n",
        "# Load the networks, and do the initial encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPUrEu9sqAq_"
      },
      "source": [
        "# need to reload nets each time after finetuning\n",
        "nets = load_nets()\n",
        "outdim = nets.setting['outdim']\n",
        "\n",
        "with torch.no_grad():\n",
        "    mask = torch.ones_like(source_im)[:, [0], :, :]\n",
        "    out = nets.invert(source_im, mask)\n",
        "    show(['Inverted Image', renormalize.as_image(out[0]).resize((256, 256), Image.LANCZOS)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWGVguNGqSXN"
      },
      "source": [
        "# Finetune the encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZFeFVIHqLJe"
      },
      "source": [
        "from networks.psp import id_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApJ-drshqM1e"
      },
      "source": [
        "# need to reload nets each time after finetuning\n",
        "nets = load_nets()\n",
        "outdim = nets.setting['outdim']\n",
        "\n",
        "with torch.no_grad():\n",
        "    mask = torch.ones_like(source_im)[:, [0], :, :]\n",
        "    initial_inversion = nets.invert(source_im, mask)\n",
        "\n",
        "batch_size = 1\n",
        "lambda_mse = 1.0\n",
        "lambda_lpips = 1.0\n",
        "lambda_z = 0. # set lambda_z to 10.0 to optimize the latent first. (optional)\n",
        "lambda_id = 0.1\n",
        "\n",
        "# do optional latent optimization\n",
        "if lambda_z > 0.:\n",
        "    checkpoint_dict, opt_losses = inversions.invert_lbfgs(nets, source_im, num_steps=30)\n",
        "    opt_ws = checkpoint_dict['current_z'].detach().clone().repeat(batch_size, 1, 1)\n",
        "    # reenable grad after LBFGS\n",
        "    torch.set_grad_enabled(True)\n",
        "\n",
        "netG = nets.generator.eval()\n",
        "# this keeps the batchnorm params fixed during finetuning, but can change this\n",
        "netE = nets.encoder.eval() \n",
        "util.set_requires_grad(False, netG)\n",
        "util.set_requires_grad(True, netE)\n",
        "\n",
        "mse_loss = torch.nn.MSELoss()\n",
        "l1_loss = torch.nn.L1Loss()\n",
        "perceptual_loss = losses.LPIPS_Loss().cuda().eval()\n",
        "identity_loss = id_loss.IDLoss().cuda().eval()\n",
        "util.set_requires_grad(False, identity_loss)\n",
        "util.set_requires_grad(False, perceptual_loss)\n",
        "\n",
        "optimizer = torch.optim.Adam(netE.parameters(), lr=0.00005, betas=(0.5, 0.999))\n",
        "\n",
        "target = source_im.repeat(batch_size, 1, 1, 1)\n",
        "\n",
        "reshape = torch.nn.AdaptiveAvgPool2d((256, 256))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnKBMrI-UuQT"
      },
      "source": [
        "all_losses = dict(z=[], mse=[], lpips=[], id=[], sim_improvement=[])\n",
        "\n",
        "# 30-50 steps is about enough, can do more steps for a better match\n",
        "torch.manual_seed(0)\n",
        "\n",
        "for i in pbar(range(30)):\n",
        "    optimizer.zero_grad()\n",
        "    mask_data = [masking.mask_upsample(source_im) for _ in range(batch_size)]\n",
        "    hints = torch.cat([m[0] for m in mask_data])\n",
        "    masks = torch.cat([m[1] for m in mask_data])\n",
        "    \n",
        "    encoded = netE(torch.cat([hints, masks], dim=1))\n",
        "    regenerated = netG(encoded)\n",
        "    if lambda_z > 0.:\n",
        "        loss_z = mse_loss(encoded, opt_ws)\n",
        "    else:\n",
        "        loss_z = torch.Tensor((0.,)).cuda()\n",
        "    loss_mse = mse_loss(regenerated, target)\n",
        "    loss_perceptual = perceptual_loss.forward(\n",
        "        reshape(regenerated), reshape(target)).mean()\n",
        "    loss_id, sim_improvement, id_logs = identity_loss(reshape(regenerated), reshape(target), reshape(target))\n",
        "    loss = (lambda_z * loss_z + lambda_mse * loss_mse\n",
        "            + lambda_lpips * loss_perceptual + lambda_id * loss_id)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    all_losses['z'].append(loss_z.item())\n",
        "    all_losses['mse'].append(loss_mse.item())\n",
        "    all_losses['lpips'].append(loss_perceptual.item())\n",
        "    all_losses['id'].append(loss_id.item())\n",
        "    all_losses['sim_improvement'].append(sim_improvement)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5MtdjYLqX4X"
      },
      "source": [
        "f, ax = plt.subplots(1,4, figsize=(16, 3))\n",
        "ax[0].plot(all_losses['z'])\n",
        "ax[0].set_title('Z loss')\n",
        "ax[1].plot(all_losses['mse'])\n",
        "ax[1].set_title('MSE loss')\n",
        "ax[2].plot(all_losses['lpips'])\n",
        "ax[2].set_title('LPIPS loss')\n",
        "ax[3].plot(all_losses['id'])\n",
        "ax[3].set_title('ID loss')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7zqK6PPqblB"
      },
      "source": [
        "show.a(['Initial Inversion', renormalize.as_image(initial_inversion[0]).resize((256, 256), Image.LANCZOS)])\n",
        "if lambda_z > 0.:\n",
        "    show.a(['optimized w', renormalize.as_image(checkpoint_dict['current_x'][0]).resize((256, 256), Image.LANCZOS)])\n",
        "show.flush()\n",
        "\n",
        "with torch.no_grad():\n",
        "    hints = source_im\n",
        "    mask = torch.ones_like(source_im)[:, [0], :, :]\n",
        "    \n",
        "    encoded = nets.encode(hints, mask)\n",
        "    out = nets.decode(encoded)\n",
        "    show.a(['hints Image', renormalize.as_image(hints[0]).resize((256, 256), Image.LANCZOS)])\n",
        "    show.a(['Inverted Image', renormalize.as_image(out[0]).resize((256, 256), Image.LANCZOS)])\n",
        "    show.flush()\n",
        "    \n",
        "    mask = torch.ones_like(source_im)[:, [0], :, :]\n",
        "    mask[:, :, 100:-100, 100:-100] = 0.\n",
        "    hints = source_im*mask\n",
        "    \n",
        "    encoded = nets.encode(hints, mask)\n",
        "    out = nets.decode(encoded)\n",
        "    show.a(['Hints Image', renormalize.as_image(hints[0]).resize((256, 256), Image.LANCZOS)])\n",
        "    show.a(['Inverted Hints', renormalize.as_image(out[0]).resize((256, 256), Image.LANCZOS)])\n",
        "    show.flush()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJ3O6V7XqwYw"
      },
      "source": [
        "# Interactive mixing\n",
        "Draw your mouse on the image panels. The network input will show in the second to last panel, and the network output in the last panel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjr8VMogqv21"
      },
      "source": [
        "collage_paths = [\n",
        "    im_path,\n",
        "    'img/efros_cropped.png',\n",
        "    'img/phil_cropped.png',\n",
        "    'img/biden_cropped.png'\n",
        "]\n",
        "num_components = len(collage_paths)\n",
        "collage_ims = torch.cat([transform(Image.open(p).convert('RGB'))[None].cuda() for p in collage_paths])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmpGE0Olq0we"
      },
      "source": [
        "from utils import paintwidget, labwidget\n",
        "\n",
        "def make_callback(painter):\n",
        "    def probe_changed(c):\n",
        "        global composite\n",
        "        global mask_composite\n",
        "        p = painter\n",
        "        if p.mask:\n",
        "            mask = renormalize.from_url(p.mask, target='pt', size=(outdim, outdim)).cuda()[None]\n",
        "        else:\n",
        "            mask = torch.zeros_like(sample)[None]\n",
        "        with torch.no_grad():\n",
        "            mask = mask[:, [0], :, :].cuda()\n",
        "            mask_composite += mask\n",
        "            sample = renormalize.from_url(p.image, size=(outdim, outdim)).cuda()[None]\n",
        "            \n",
        "            composite = sample * mask + composite * (1-mask)\n",
        "            mask_composite = torch.clamp(mask_composite, 0., 1.)\n",
        "            out = nets.invert(composite, mask_composite)\n",
        "        img_url = renormalize.as_url(composite[0], size=256)\n",
        "        img_html = '<img src=\"%s\"/>'% img_url\n",
        "        collage_div.innerHTML = img_html   \n",
        "        img_url = renormalize.as_url(out[0], size=256)\n",
        "        img_html = '<img src=\"%s\"/>'% img_url\n",
        "        encoded_div.innerHTML = img_html\n",
        "    return probe_changed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCHHpuRoq284"
      },
      "source": [
        "img_url = renormalize.as_url(torch.zeros(3, outdim, outdim), size=256)\n",
        "img_html = '<img src=\"%s\"/>'%img_url\n",
        "encoded_div = labwidget.Div(img_html)\n",
        "collage_div = labwidget.Div(img_html)\n",
        "\n",
        "painters = []\n",
        "\n",
        "composite = torch.zeros(1, 3, outdim, outdim).cuda()\n",
        "mask_composite = torch.zeros_like(composite)[:, [0], :, :]\n",
        "\n",
        "for i in range(num_components):\n",
        "    src_painter = paintwidget.PaintWidget(oneshot=False, width=256, height=256, \n",
        "                                      brushsize=20, save_sequence=False, track_move=True) # , on_move=True)\n",
        "    src_painter.image = renormalize.as_url(collage_ims[i], size=256)\n",
        "    painters.append(src_painter)\n",
        "    callback = make_callback(src_painter)\n",
        "    src_painter.on('mask', callback)\n",
        "    show.a([src_painter], cols=3)\n",
        "\n",
        "show.a([collage_div], cols=3)\n",
        "show.a([encoded_div], cols=3)\n",
        "show.flush()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-uYttdAjcdv"
      },
      "source": [
        "# Show the result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6OUcxh-i5ZP"
      },
      "source": [
        "def show_drawing():\n",
        "    for i, p in enumerate(painters):\n",
        "        if p.mask:\n",
        "            mask = renormalize.from_url(p.mask, target='pt', size=(outdim, outdim)).cuda()[None]\n",
        "        else:\n",
        "            mask = torch.zeros(1, 3, outdim, outdim).cuda()\n",
        "        mask = mask[:, [0], :, :].cuda()\n",
        "        sample = renormalize.from_url(p.image, size=(outdim, outdim)).cuda()[None]\n",
        "        part = sample * mask\n",
        "        im_pil = imutil.draw_masked_image(sample, mask, size=256)[1]\n",
        "        # im_pil.save(os.path.join(save_path, 'part%d.png' % i))\n",
        "        show.a(['part %d' % i, im_pil.resize((200, 200), Image.ANTIALIAS)], cols=3)\n",
        "    with torch.no_grad():\n",
        "        out = nets.invert(composite, mask_composite)\n",
        "    composite_pil = renormalize.as_image(out[0])\n",
        "    input_np = np.array(renormalize.as_image(composite[0]))\n",
        "    mask_np = np.stack([np.array(mask_composite.cpu()[0][0])] * 3, axis=2)\n",
        "    input_np[mask_np == 0] = 255//2# 200 # lighten the unfilled region\n",
        "    input_pil = Image.fromarray(input_np)\n",
        "    # input_pil = renormalize.as_image(composite[0])\n",
        "    # composite_pil.save(os.path.join(save_path, 'composite.png'))\n",
        "    show.a(['input', input_pil.resize((200, 200), Image.ANTIALIAS)])\n",
        "    show.a(['composite', composite_pil.resize((200, 200), Image.ANTIALIAS)])\n",
        "    show.flush()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwZJkcNUgpjl"
      },
      "source": [
        "show_drawing()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
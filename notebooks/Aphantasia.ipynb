{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Aphantasia.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN7YTOMER36Bt0iitHDDstF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/16A0/til/blob/master/Aphantasia.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "id": "W69TXNGDfW5w",
        "outputId": "e3724f89-f002-4c87-a028-ad041573ee7e"
      },
      "source": [
        "import subprocess\n",
        "CUDA_version = [s for s in subprocess.check_output([\"nvcc\", \"--version\"]).decode(\"UTF-8\").split(\", \") if s.startswith(\"release\")][0].split(\" \")[-1]\n",
        "print(\"CUDA version:\", CUDA_version)\n",
        "\n",
        "if CUDA_version == \"10.0\":\n",
        "    torch_version_suffix = \"+cu100\"\n",
        "elif CUDA_version == \"10.1\":\n",
        "    torch_version_suffix = \"+cu101\"\n",
        "elif CUDA_version == \"10.2\":\n",
        "    torch_version_suffix = \"\"\n",
        "else:\n",
        "    torch_version_suffix = \"+cu110\"\n",
        "\n",
        "!pip install torch==1.7.1{torch_version_suffix} torchvision==0.8.2{torch_version_suffix} -f https://download.pytorch.org/whl/torch_stable.html ftfy regex\n",
        "\n",
        "try: \n",
        "  !pip3 install googletrans==3.1.0a0\n",
        "  from googletrans import Translator, constants\n",
        "  # from pprint import pprint\n",
        "  translator = Translator()\n",
        "except: pass\n",
        "!pip install ftfy==5.8\n",
        "\n",
        "import os\n",
        "import io\n",
        "import time\n",
        "from math import exp\n",
        "import random\n",
        "import imageio\n",
        "import numpy as np\n",
        "import PIL\n",
        "from skimage import exposure\n",
        "from base64 import b64encode\n",
        "import moviepy, moviepy.editor\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from IPython.display import HTML, Image, display, clear_output\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n",
        "import ipywidgets as ipy\n",
        "# import glob\n",
        "from google.colab import output, files\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "import clip\n",
        "!pip install sentence_transformers\n",
        "from sentence_transformers import SentenceTransformer\n",
        "!pip install git+https://github.com/Po-Hsun-Su/pytorch-ssim\n",
        "import pytorch_ssim as ssim\n",
        "\n",
        "%cd /content\n",
        "!rm -rf aphantasia\n",
        "!git clone https://github.com/eps696/aphantasia\n",
        "%cd aphantasia/\n",
        "from clip_fft import to_valid_rgb, fft_image, slice_imgs, checkout\n",
        "from utils import pad_up_to, basename, img_list, img_read\n",
        "from progress_bar import ProgressIPy as ProgressBar\n",
        "\n",
        "workdir = '_out'\n",
        "tempdir = os.path.join(workdir, 'ttt')\n",
        "\n",
        "clear_output()\n",
        "\n",
        "resume = True #@param {type:\"boolean\"}\n",
        "if resume:\n",
        "  resumed = files.upload()\n",
        "  params_pt = list(resumed.values())[0]\n",
        "  params_pt = torch.load(io.BytesIO(params_pt))\n",
        "\n",
        "def makevid(seq_dir, size=None):\n",
        "  # out_sequence = seq_dir + '/%05d.jpg'\n",
        "  out_video = seq_dir + '.mp4'\n",
        "  # !ffmpeg -y -v quiet -i $out_sequence $out_video\n",
        "  moviepy.editor.ImageSequenceClip(img_list(seq_dir), fps=25).write_videofile(out_video, verbose=False) # , ffmpeg_params=ffmpeg_params, logger=None\n",
        "  data_url = \"data:video/mp4;base64,\" + b64encode(open(out_video,'rb').read()).decode()\n",
        "  wh = '' if size is None else 'width=%d height=%d' % (size, size)\n",
        "  return \"\"\"<video %s controls><source src=\"%s\" type=\"video/mp4\"></video>\"\"\" % (wh, data_url)\n",
        "\n",
        "!nvidia-smi -L\n",
        "print('\\nDone!')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d5c01b09-4152-4408-9406-eaf12ff8f2ed\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-d5c01b09-4152-4408-9406-eaf12ff8f2ed\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-235d9037840e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0mresume\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;31m#@param {type:\"boolean\"}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mresume\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m   \u001b[0mresumed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m   \u001b[0mparams_pt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresumed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m   \u001b[0mparams_pt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams_pt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m   result = _output.eval_js(\n\u001b[1;32m     63\u001b[0m       'google.colab._files._uploadFiles(\"{input_id}\", \"{output_id}\")'.format(\n\u001b[0;32m---> 64\u001b[0;31m           input_id=input_id, output_id=output_id))\n\u001b[0m\u001b[1;32m     65\u001b[0m   \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_collections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_six\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    104\u001b[0m         reply.get('colab_msg_id') == message_id):\n\u001b[1;32m    105\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: TypeError: Cannot read property '_uploadFiles' of undefined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_VMCq-ZgfD2"
      },
      "source": [
        "Source https://github.com/eps696/aphantasia/blob/master/Aphantasia.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7D9YzmQf_bJ"
      },
      "source": [
        "\n",
        "Type some text and/or upload some image to start.\n",
        "fine_details input would make micro details follow that topic.\n",
        "Put to subtract the topics, which you would like to avoid in the result.\n",
        "NB: more prompts = more memory! (handled by auto-decreasing samples amount, hopefully you don't need to act).\n",
        "invert the whole criteria, if you want to see \"the totally opposite\".\n",
        "\n",
        "Options for non-English languages (use only one of them!):\n",
        "multilang = use multi-language model, trained with ViT\n",
        "translate = use Google translate (works with any visual model)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "2xzyYhv5gCHn"
      },
      "source": [
        "#@title Input\n",
        "\n",
        "text = \"sunrise on town at the edge of ocean cliff\" #@param {type:\"string\"}\n",
        "fine_details = \"scottish highland \" #@param {type:\"string\"}\n",
        "subtract = \"\" #@param {type:\"string\"}\n",
        "multilang = False #@param {type:\"boolean\"}\n",
        "translate = False #@param {type:\"boolean\"}\n",
        "invert = False #@param {type:\"boolean\"}\n",
        "upload_image = False #@param {type:\"boolean\"}\n",
        "\n",
        "if translate:\n",
        "  text = translator.translate(text, dest='en').text\n",
        "if upload_image:\n",
        "  uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWX8sgPmgHZP"
      },
      "source": [
        "Settings\n",
        "Select CLIP visual model (results do vary!). I prefer ViT for consistency (and it's the only native multi-language option).\n",
        "overscan option produces semi-seamlessly tileable texture (when off, it's more centered).\n",
        "sync value adds SSIM loss between the output and input image (if there's one), allowing to \"redraw\" it with controlled similarity.\n",
        "\n",
        "Decrease samples if you face OOM (it's the main RAM eater).\n",
        "Setting steps much higher (1000-..) will elaborate details and make tones smoother, but may start throwing texts like graffiti.\n",
        "\n",
        "Other tricks:\n",
        "diverse boosts compositional & contextual variety (difference between simultaneous samples). good start is ~0.2; can be negative.\n",
        "expand boosts training steps in general (difference between consequent samples). good start is ~0.2.\n",
        "progressive_grow may boost macro forms creation (especially with lower learning_rate), see more [here](https://github.com/eps696/aphantasia/issues/2)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09q5wYd1gTqH"
      },
      "source": [
        "#@title Generate\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/GDrive')\n",
        "# clipsDir = '/content/GDrive/MyDrive/T2I ' + dtNow.strftime(\"%Y-%m-%d %H%M\")\n",
        "\n",
        "!rm -rf $tempdir\n",
        "os.makedirs(tempdir, exist_ok=True)\n",
        "\n",
        "sideX = 1280 #@param {type:\"integer\"}\n",
        "sideY = 720 #@param {type:\"integer\"}\n",
        "#@markdown > Config\n",
        "model = 'ViT-B/32' #@param ['ViT-B/32', 'RN101', 'RN50x4', 'RN50']\n",
        "overscan = True #@param {type:\"boolean\"}\n",
        "sync =  0.5 #@param {type:\"number\"}\n",
        "contrast = 1. #@param {type:\"number\"}\n",
        "#@markdown > Training\n",
        "steps = 200 #@param {type:\"integer\"}\n",
        "samples = 200 #@param {type:\"integer\"}\n",
        "learning_rate = .05 #@param {type:\"number\"}\n",
        "save_freq = 1 #@param {type:\"integer\"}\n",
        "#@markdown > Tricks\n",
        "diverse = 0. #@param {type:\"number\"}\n",
        "expand = 0. #@param {type:\"number\"}\n",
        "progressive_grow = False #@param {type:\"boolean\"}\n",
        "if multilang: model = 'ViT-B/32' # sbert model is trained with ViT\n",
        "\n",
        "if len(fine_details) > 0:\n",
        "  samples = int(samples * 0.75)\n",
        "if len(subtract) > 0:\n",
        "  samples = int(samples * 0.75)\n",
        "print(' using %d samples' % samples)\n",
        "\n",
        "model_clip, _ = clip.load(model)\n",
        "modsize = 288 if model == 'RN50x4' else 224\n",
        "xmem = {'RN50':0.5, 'RN50x4':0.16, 'RN101':0.33}\n",
        "if 'RN' in model:\n",
        "  samples = int(samples * xmem[model])\n",
        "\n",
        "if multilang is True:\n",
        "    model_lang = SentenceTransformer('clip-ViT-B-32-multilingual-v1').cuda()\n",
        "\n",
        "def enc_text(txt):\n",
        "    if multilang is True:\n",
        "        emb = model_lang.encode([txt], convert_to_tensor=True, show_progress_bar=False)\n",
        "    else:\n",
        "        emb = model_clip.encode_text(clip.tokenize(txt).cuda())\n",
        "    return emb.detach().clone()\n",
        "\n",
        "if diverse != 0:\n",
        "  samples = int(samples * 0.5)\n",
        "        \n",
        "norm_in = torchvision.transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
        "sign = 1. if invert is True else -1.\n",
        "\n",
        "if upload_image:\n",
        "  in_img = list(uploaded.values())[0]\n",
        "  print(' image:', list(uploaded)[0])\n",
        "  img_in = torch.from_numpy(imageio.imread(in_img).astype(np.float32)/255.).unsqueeze(0).permute(0,3,1,2).cuda()[:,:3,:,:]\n",
        "  in_sliced = slice_imgs([img_in], samples, modsize, transform=norm_in)[0]\n",
        "  img_enc = model_clip.encode_image(in_sliced).detach().clone()\n",
        "  if sync > 0:\n",
        "    overscan = True\n",
        "    ssim_loss = ssim.SSIM(window_size = 11)\n",
        "    ssim_size = [sideY//8, sideX//8]\n",
        "    img_in = F.interpolate(img_in, ssim_size).float()\n",
        "    # img_in = F.interpolate(img_in, (sideY, sideX)).float()\n",
        "  else:\n",
        "    del img_in\n",
        "  del in_sliced; torch.cuda.empty_cache()\n",
        "\n",
        "if len(text) > 2:\n",
        "  print(' macro:', text)\n",
        "  if translate:\n",
        "    translator = Translator()\n",
        "    text = translator.translate(text, dest='en').text\n",
        "    print(' translated to:', text) \n",
        "  txt_enc = enc_text(text)\n",
        "\n",
        "if len(fine_details) > 0:\n",
        "  print(' micro:', fine_details)\n",
        "  if translate:\n",
        "      translator = Translator()\n",
        "      fine_details = translator.translate(fine_details, dest='en').text\n",
        "      print(' translated to:', fine_details) \n",
        "  txt_enc2 = enc_text(fine_details)\n",
        "\n",
        "if len(subtract) > 0:\n",
        "  print(' without:', subtract)\n",
        "  if translate:\n",
        "      translator = Translator()\n",
        "      subtract = translator.translate(subtract, dest='en').text\n",
        "      print(' translated to:', subtract) \n",
        "  txt_enc0 = enc_text(subtract)\n",
        "\n",
        "if multilang is True: del model_lang\n",
        "\n",
        "shape = [1, 3, sideY, sideX]\n",
        "param_f = fft_image \n",
        "# param_f = pixel_image\n",
        "# learning_rate = 1.\n",
        "init_pt = params_pt if resume is True else None\n",
        "params, image_f = param_f(shape, resume=init_pt)\n",
        "image_f = to_valid_rgb(image_f)\n",
        "\n",
        "if progressive_grow is True:\n",
        "  lr1 = learning_rate * 2\n",
        "  lr0 = lr1 * 0.01\n",
        "else:\n",
        "  lr0 = learning_rate\n",
        "optimizer = torch.optim.Adam(params, lr0)\n",
        "\n",
        "def save_img(img, fname=None):\n",
        "  img = np.array(img)[:,:,:]\n",
        "  img = np.transpose(img, (1,2,0))  \n",
        "  img = np.clip(img*255, 0, 255).astype(np.uint8)\n",
        "  if fname is not None:\n",
        "    imageio.imsave(fname, np.array(img))\n",
        "    imageio.imsave('result.jpg', np.array(img))\n",
        "\n",
        "def checkout(num):\n",
        "  with torch.no_grad():\n",
        "    img = image_f(contrast=contrast).cpu().numpy()[0]\n",
        "  save_img(img, os.path.join(tempdir, '%04d.jpg' % num))\n",
        "  outpic.clear_output()\n",
        "  with outpic:\n",
        "    display(Image('result.jpg'))\n",
        "\n",
        "prev_enc = 0\n",
        "def train(i):\n",
        "  loss = 0\n",
        "  img_out = image_f()\n",
        "\n",
        "  micro = False if len(fine_details) > 0 else None\n",
        "  imgs_sliced = slice_imgs([img_out], samples, modsize, norm_in, overscan=overscan, micro=micro)\n",
        "  out_enc = model_clip.encode_image(imgs_sliced[-1])\n",
        "  if diverse != 0:\n",
        "    imgs_sliced = slice_imgs([image_f()], samples, modsize, norm_in, overscan=overscan, micro=micro)\n",
        "    out_enc2 = model_clip.encode_image(imgs_sliced[-1])\n",
        "    loss += diverse * torch.cosine_similarity(out_enc, out_enc2, dim=-1).mean()\n",
        "    del out_enc2; torch.cuda.empty_cache()\n",
        "  if upload_image:\n",
        "      loss += sign * 0.5 * torch.cosine_similarity(img_enc, out_enc, dim=-1).mean()\n",
        "  if len(text) > 0: # input text\n",
        "      loss += sign * torch.cosine_similarity(txt_enc, out_enc, dim=-1).mean()\n",
        "  if len(subtract) > 0: # subtract text\n",
        "      loss += -sign * torch.cosine_similarity(txt_enc0, out_enc, dim=-1).mean()\n",
        "  if sync > 0 and upload_image: # image composition sync\n",
        "      loss -= sync * ssim_loss(F.interpolate(img_out, ssim_size).float(), img_in)\n",
        "  if len(fine_details) > 0: # input text for micro details\n",
        "      imgs_sliced = slice_imgs([img_out], samples, modsize, norm_in, overscan=overscan, micro=True)\n",
        "      out_enc2 = model_clip.encode_image(imgs_sliced[-1])\n",
        "      loss += sign * torch.cosine_similarity(txt_enc2, out_enc2, dim=-1).mean()\n",
        "      del out_enc2; torch.cuda.empty_cache()\n",
        "  if expand > 0:\n",
        "    global prev_enc\n",
        "    if i > 0:\n",
        "      loss += expand * torch.cosine_similarity(out_enc, prev_enc, dim=-1).mean()\n",
        "    prev_enc = out_enc.detach()\n",
        "  del img_out, imgs_sliced, out_enc; torch.cuda.empty_cache()\n",
        "\n",
        "  if progressive_grow is True:\n",
        "    lr_cur = lr0 + (i / steps) * (lr1 - lr0)\n",
        "    for g in optimizer.param_groups: \n",
        "      g['lr'] = lr_cur\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  \n",
        "  if i % save_freq == 0:\n",
        "    checkout(i // save_freq)\n",
        "\n",
        "outpic = ipy.Output()\n",
        "outpic\n",
        "\n",
        "pbar = ProgressBar(steps)\n",
        "for i in range(steps):\n",
        "  train(i)\n",
        "  _ = pbar.upd()\n",
        "\n",
        "HTML(makevid(tempdir))\n",
        "torch.save(params, tempdir + '.pt')\n",
        "files.download(tempdir + '.pt')\n",
        "files.download(tempdir + '.mp4')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
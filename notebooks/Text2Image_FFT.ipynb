{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Text2Image FFT.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/16A0/til/blob/master/Text2Image_FFT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toWe1IoH7X35"
      },
      "source": [
        "# Text to Image\r\n",
        "\r\n",
        "Based on [CLIP](https://github.com/openai/CLIP) + FFT from [Lucent](https://github.com/greentfrapp/lucent) // made by [eps696](https://github.com/eps696)\r\n",
        "\r\n",
        "## Features \r\n",
        "* uses image and/or text as prompts\r\n",
        "* generates [FFT-encoded](https://github.com/greentfrapp/lucent/blob/master/lucent/optvis/param/spatial.py) image (detailed tiled textures, a la deepdream)\r\n",
        "* ! very fast convergence\r\n",
        "* ! undemanding for RAM (fullHD resolution and more)\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QytcEMSKBtN-"
      },
      "source": [
        "**Run this cell after each session restart**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etzxXVZ_r-Nf",
        "cellView": "form"
      },
      "source": [
        "#@title General setup\r\n",
        "\r\n",
        "import subprocess\r\n",
        "CUDA_version = [s for s in subprocess.check_output([\"nvcc\", \"--version\"]).decode(\"UTF-8\").split(\", \") if s.startswith(\"release\")][0].split(\" \")[-1]\r\n",
        "print(\"CUDA version:\", CUDA_version)\r\n",
        "\r\n",
        "if CUDA_version == \"10.0\":\r\n",
        "    torch_version_suffix = \"+cu100\"\r\n",
        "elif CUDA_version == \"10.1\":\r\n",
        "    torch_version_suffix = \"+cu101\"\r\n",
        "elif CUDA_version == \"10.2\":\r\n",
        "    torch_version_suffix = \"\"\r\n",
        "else:\r\n",
        "    torch_version_suffix = \"+cu110\"\r\n",
        "\r\n",
        "!pip install torch==1.7.1{torch_version_suffix} torchvision==0.8.2{torch_version_suffix} -f https://download.pytorch.org/whl/torch_stable.html ftfy regex\r\n",
        "\r\n",
        "try: \r\n",
        "  !pip3 install googletrans==3.1.0a0\r\n",
        "  from googletrans import Translator, constants\r\n",
        "  # from pprint import pprint\r\n",
        "  translator = Translator()\r\n",
        "except: pass\r\n",
        "!pip install ftfy\r\n",
        "\r\n",
        "import os\r\n",
        "import time\r\n",
        "import random\r\n",
        "import imageio\r\n",
        "import numpy as np\r\n",
        "import PIL\r\n",
        "from skimage import exposure\r\n",
        "from base64 import b64encode\r\n",
        "\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torchvision\r\n",
        "\r\n",
        "from IPython.display import HTML, Image, display, clear_output\r\n",
        "from IPython.core.interactiveshell import InteractiveShell\r\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\r\n",
        "import ipywidgets as ipy\r\n",
        "# import glob\r\n",
        "from google.colab import output, files\r\n",
        "\r\n",
        "import warnings\r\n",
        "warnings.filterwarnings(\"ignore\")\r\n",
        "\r\n",
        "!git clone https://github.com/openai/CLIP.git\r\n",
        "%cd /content/CLIP/\r\n",
        "import clip\r\n",
        "perceptor, preprocess = clip.load('ViT-B/32')\r\n",
        "\r\n",
        "workdir = '_out'\r\n",
        "tempdir = os.path.join(workdir, 'ttt')\r\n",
        "os.makedirs(tempdir, exist_ok=True)\r\n",
        "\r\n",
        "clear_output()\r\n",
        "\r\n",
        "###  FFT from Lucent library  https://github.com/greentfrapp/lucent\r\n",
        "\r\n",
        "def pixel_image(shape, sd=2.):\r\n",
        "    tensor = (torch.randn(*shape) * sd).cuda().requires_grad_(True)\r\n",
        "    return [tensor], lambda: tensor\r\n",
        "\r\n",
        "# From https://github.com/tensorflow/lucid/blob/master/lucid/optvis/param/spatial.py\r\n",
        "def rfft2d_freqs(h, w):\r\n",
        "    \"\"\"Computes 2D spectrum frequencies.\"\"\"\r\n",
        "    fy = np.fft.fftfreq(h)[:, None]\r\n",
        "    # when we have an odd input dimension we need to keep one additional frequency and later cut off 1 pixel\r\n",
        "    if w % 2 == 1:\r\n",
        "        fx = np.fft.fftfreq(w)[: w // 2 + 2]\r\n",
        "    else:\r\n",
        "        fx = np.fft.fftfreq(w)[: w // 2 + 1]\r\n",
        "    return np.sqrt(fx * fx + fy * fy)\r\n",
        "\r\n",
        "def fft_image(shape, sd=0.1, decay_power=1., smooth_col=1.):\r\n",
        "    batch, channels, h, w = shape\r\n",
        "    freqs = rfft2d_freqs(h, w)\r\n",
        "    init_val_size = (batch, channels) + freqs.shape + (2,) # 2 for imaginary and real components\r\n",
        "    spectrum_real_imag_t = (torch.randn(*init_val_size) * sd).cuda().requires_grad_(True)\r\n",
        "    scale = 1.0 / np.maximum(freqs, 1.0 / max(w, h)) ** decay_power\r\n",
        "    scale = torch.tensor(scale).float()[None, None, ..., None].cuda()\r\n",
        "\r\n",
        "    def inner():\r\n",
        "        scaled_spectrum_t = scale * spectrum_real_imag_t\r\n",
        "        image = torch.irfft(scaled_spectrum_t, 2, normalized=True, signal_sizes=(h, w))\r\n",
        "        image = image[:batch, :channels, :h, :w]\r\n",
        "        image = image / smooth_col # reduce saturation, smoothen colors & contrast\r\n",
        "        return image\r\n",
        "    return [spectrum_real_imag_t], inner\r\n",
        "\r\n",
        "def to_valid_rgb(image_f, decorrelate=True):\r\n",
        "    def inner():\r\n",
        "        image = image_f()\r\n",
        "        if decorrelate:\r\n",
        "            image = _linear_decorrelate_color(image)\r\n",
        "        return torch.sigmoid(image)\r\n",
        "    return inner\r\n",
        "    \r\n",
        "def _linear_decorrelate_color(tensor):\r\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "    t_permute = tensor.permute(0,2,3,1)\r\n",
        "    t_permute = torch.matmul(t_permute, torch.tensor(color_correlation_normalized.T).to(device))\r\n",
        "    tensor = t_permute.permute(0,3,1,2)\r\n",
        "    return tensor\r\n",
        "\r\n",
        "color_correlation_svd_sqrt = np.asarray([[0.26, 0.09, 0.02],\r\n",
        "                                         [0.27, 0.00, -0.05],\r\n",
        "                                         [0.27, -0.09, 0.03]]).astype(\"float32\")\r\n",
        "max_norm_svd_sqrt = np.max(np.linalg.norm(color_correlation_svd_sqrt, axis=0))\r\n",
        "color_correlation_normalized = color_correlation_svd_sqrt / max_norm_svd_sqrt\r\n",
        "\r\n",
        "### Libs\r\n",
        "\r\n",
        "def slice_imgs(imgs, count, transform=None, uniform=True):\r\n",
        "  def map(x, a, b):\r\n",
        "    return x * (b-a) + a\r\n",
        "  rnd_size = torch.rand(count)\r\n",
        "  if uniform is True:\r\n",
        "    rnd_offx = torch.rand(count)\r\n",
        "    rnd_offy = torch.rand(count)\r\n",
        "  else: # ~normal around center\r\n",
        "    rnd_offx = torch.clip(torch.randn(count) * 0.2 + 0.5, 0, 1) \r\n",
        "    rnd_offy = torch.clip(torch.randn(count) * 0.2 + 0.5, 0, 1)\r\n",
        "  \r\n",
        "  sz = [img.shape[2:] for img in imgs]\r\n",
        "  sz_min = [np.min(s) for s in sz]\r\n",
        "  if uniform is True:\r\n",
        "    sz = [[2*s[0], 2*s[1]] for s in list(sz)]\r\n",
        "    imgs = [pad_up_to(imgs[i], sz[i], type='centr') for i in range(len(imgs))]\r\n",
        "\r\n",
        "  sliced = []\r\n",
        "  for i, img in enumerate(imgs):\r\n",
        "    cuts = []\r\n",
        "    for c in range(count):\r\n",
        "      csize = map(rnd_size[c], 224, 0.98*sz_min[i]).int()\r\n",
        "      offsetx = map(rnd_offx[c], 0, sz[i][1] - csize).int()\r\n",
        "      offsety = map(rnd_offy[c], 0, sz[i][0] - csize).int()\r\n",
        "      cut = img[:, :, offsety:offsety + csize, offsetx:offsetx + csize]\r\n",
        "      cut = torch.nn.functional.interpolate(cut, (224,224), mode='bilinear')\r\n",
        "      if transform is not None: \r\n",
        "          cut = transform(cut)\r\n",
        "      cuts.append(cut)\r\n",
        "    sliced.append(torch.cat(cuts, 0))\r\n",
        "  return sliced\r\n",
        "\r\n",
        "def makevid(seq_dir, size=None):\r\n",
        "  out_sequence = seq_dir + '/%03d.jpg'\r\n",
        "  out_video = seq_dir + '.mp4'\r\n",
        "  !ffmpeg -y -v warning -i $out_sequence $out_video\r\n",
        "  data_url = \"data:video/mp4;base64,\" + b64encode(open(out_video,'rb').read()).decode()\r\n",
        "  wh = '' if size is None else 'width=%d height=%d' % (size, size)\r\n",
        "  return \"\"\"<video %s controls><source src=\"%s\" type=\"video/mp4\"></video>\"\"\" % (wh, data_url)\r\n",
        "\r\n",
        "# Tiles an array around two points, allowing for pad lengths greater than the input length\r\n",
        "# adapted from https://discuss.pytorch.org/t/symmetric-padding/19866/3\r\n",
        "def tile_pad(xt, padding):\r\n",
        "  h, w = xt.shape[-2:]\r\n",
        "  left, right, top, bottom = padding\r\n",
        "\r\n",
        "  def tile(x, minx, maxx):\r\n",
        "    rng = maxx - minx\r\n",
        "    mod = np.remainder(x - minx, rng)\r\n",
        "    out = mod + minx\r\n",
        "    return np.array(out, dtype=x.dtype)\r\n",
        "\r\n",
        "  x_idx = np.arange(-left, w+right)\r\n",
        "  y_idx = np.arange(-top, h+bottom)\r\n",
        "  x_pad = tile(x_idx, -0.5, w-0.5)\r\n",
        "  y_pad = tile(y_idx, -0.5, h-0.5)\r\n",
        "  xx, yy = np.meshgrid(x_pad, y_pad)\r\n",
        "  return xt[..., yy, xx]\r\n",
        "\r\n",
        "def pad_up_to(x, size, type='centr'):\r\n",
        "  sh = x.shape[2:][::-1]\r\n",
        "  if list(x.shape[2:]) == list(size): return x\r\n",
        "  padding = []\r\n",
        "  for i, s in enumerate(size[::-1]):\r\n",
        "    if 'side' in type.lower():\r\n",
        "      padding = padding + [0, s-sh[i]]\r\n",
        "    else: # centr\r\n",
        "      p0 = (s-sh[i]) // 2\r\n",
        "      p1 = s-sh[i] - p0\r\n",
        "      padding = padding + [p0,p1]\r\n",
        "  y = tile_pad(x, padding)\r\n",
        "  return y\r\n",
        "\r\n",
        "class ProgressBar(object):\r\n",
        "  def __init__(self, task_num=10):\r\n",
        "    self.pbar = ipy.IntProgress(min=0, max=task_num, bar_style='') # (value=0, min=0, max=max, step=1, description=description, bar_style='')\r\n",
        "    self.labl = ipy.Label()\r\n",
        "    display(ipy.HBox([self.pbar, self.labl]))\r\n",
        "    self.task_num = task_num\r\n",
        "    self.completed = 0\r\n",
        "    self.start()\r\n",
        "\r\n",
        "  def start(self, task_num=None):\r\n",
        "    if task_num is not None:\r\n",
        "      self.task_num = task_num\r\n",
        "    if self.task_num > 0:\r\n",
        "      self.labl.value = '0/{}'.format(self.task_num)\r\n",
        "    else:\r\n",
        "      self.labl.value = 'completed: 0, elapsed: 0s'\r\n",
        "    self.start_time = time.time()\r\n",
        "\r\n",
        "  def upd(self, *p, **kw):\r\n",
        "    self.completed += 1\r\n",
        "    elapsed = time.time() - self.start_time + 0.0000000000001\r\n",
        "    fps = self.completed / elapsed if elapsed>0 else 0\r\n",
        "    if self.task_num > 0:\r\n",
        "      finaltime = time.asctime(time.localtime(self.start_time + self.task_num * elapsed / float(self.completed)))\r\n",
        "      fin = ' end %s' % finaltime[11:16]\r\n",
        "      percentage = self.completed / float(self.task_num)\r\n",
        "      eta = int(elapsed * (1 - percentage) / percentage + 0.5)\r\n",
        "      self.labl.value = '{}/{}, rate {:.3g}s, time {}s, left {}s, {}'.format(self.completed, self.task_num, 1./fps, shortime(elapsed), shortime(eta), fin)\r\n",
        "    else:\r\n",
        "      self.labl.value = 'completed {}, time {}s, {:.1f} steps/s'.format(self.completed, int(elapsed + 0.5), fps)\r\n",
        "    self.pbar.value += 1\r\n",
        "    if self.completed == self.task_num: self.pbar.bar_style = 'success'\r\n",
        "    return \r\n",
        "    # return self.completed\r\n",
        "\r\n",
        "def time_days(sec):\r\n",
        "  return '%dd %d:%02d:%02d' % (sec/86400, (sec/3600)%24, (sec/60)%60, sec%60)\r\n",
        "def time_hrs(sec):\r\n",
        "  return '%d:%02d:%02d' % (sec/3600, (sec/60)%60, sec%60)\r\n",
        "def shortime(sec):\r\n",
        "  if sec < 60:\r\n",
        "    time_short = '%d' % (sec)\r\n",
        "  elif sec < 3600:\r\n",
        "    time_short  = '%d:%02d' % ((sec/60)%60, sec%60)\r\n",
        "  elif sec < 86400:\r\n",
        "    time_short  = time_hrs(sec)\r\n",
        "  else:\r\n",
        "    time_short = time_days(sec)\r\n",
        "  return time_short\r\n",
        "\r\n",
        "\r\n",
        "!nvidia-smi -L\r\n",
        "print('\\nDone!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbJ9K4Cq8MtB"
      },
      "source": [
        "Type some text to hallucinate it, or upload some image to neuremix it.  \r\n",
        "Or use both, why not."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUvpdy8BWGuM",
        "cellView": "form"
      },
      "source": [
        "#@title Input\r\n",
        "\r\n",
        "text = \"\" #@param {type:\"string\"}\r\n",
        "translate = False #@param {type:\"boolean\"}\r\n",
        "#@markdown or \r\n",
        "upload_image = False #@param {type:\"boolean\"}\r\n",
        "\r\n",
        "if translate:\r\n",
        "  text = translator.translate(text, dest='en').text\r\n",
        "if upload_image:\r\n",
        "  uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3Sj0fxmtw6K"
      },
      "source": [
        "This method converges pretty fast, yet you may set more iterations just to get that nice animation (the result may get better as well).  \r\n",
        "`smooth_col` scaler desaturates image, while uncovering more details (the bigger the smoother)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nq0wA-wc-P-s"
      },
      "source": [
        "#@title Generate\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/GDrive')\n",
        "# clipsDir = '/content/GDrive/MyDrive/T2I ' + dtNow.strftime(\"%Y-%m-%d %H%M\")\n",
        "\n",
        "!rm -rf tempdir\n",
        "\n",
        "sideX = 1280 #@param {type:\"integer\"}\n",
        "sideY = 720 #@param {type:\"integer\"}\n",
        "smooth_col =  2.#@param {type:\"number\"}\n",
        "uniform = True #@param {type:\"boolean\"}\n",
        "#@markdown > Training\n",
        "steps = 100 #@param {type:\"integer\"}\n",
        "samples = 100 #@param {type:\"integer\"}\n",
        "learning_rate = .05 #@param {type:\"number\"}\n",
        "#@markdown > Misc\n",
        "save_freq = 1 #@param {type:\"integer\"}\n",
        "audio_notification = False #@param {type:\"boolean\"}\n",
        "\n",
        "norm_in = torchvision.transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
        "\n",
        "if upload_image:\n",
        "  input = list(uploaded.values())[0]\n",
        "  print(list(uploaded)[0])\n",
        "  img_in = torch.from_numpy(imageio.imread(input).astype(np.float32)/255.).unsqueeze(0).permute(0,3,1,2).cuda()\n",
        "  in_sliced = slice_imgs([img_in], samples, transform=norm_in)[0]\n",
        "  img_enc = perceptor.encode_image(in_sliced).detach().clone()\n",
        "  del img_in, in_sliced; torch.cuda.empty_cache()\n",
        "\n",
        "if len(text) > 2:\n",
        "  print(text)\n",
        "  if translate:\n",
        "    translator = Translator()\n",
        "    text = translator.translate(text, dest='en').text\n",
        "    print(' translated to:', text) \n",
        "  tx = clip.tokenize(text)\n",
        "  txt_enc = perceptor.encode_text(tx.cuda()).detach().clone()\n",
        "\n",
        "shape = [1, 3, sideY, sideX]\n",
        "param_f = fft_image \n",
        "# param_f = pixel_image\n",
        "# learning_rate = 1.\n",
        "params, image_f = param_f(shape, smooth_col=smooth_col)\n",
        "image_f = to_valid_rgb(image_f)\n",
        "optimizer = torch.optim.Adam(params, learning_rate)\n",
        "\n",
        "def displ(img, fname=None):\n",
        "  img = np.array(img)[:,:,:]\n",
        "  img = np.transpose(img, (1,2,0))  \n",
        "  img = exposure.equalize_adapthist(np.clip(img, -1., 1.))\n",
        "  img = np.clip(img*255, 0, 255).astype(np.uint8)\n",
        "  if fname is not None:\n",
        "    imageio.imsave(fname, np.array(img))\n",
        "    imageio.imsave('result.jpg', np.array(img))\n",
        "\n",
        "def checkin(num):\n",
        "  with torch.no_grad():\n",
        "    img = image_f().cpu().numpy()[0]\n",
        "  displ(img, os.path.join(tempdir, '%03d.jpg' % num))\n",
        "  outpic.clear_output()\n",
        "  with outpic:\n",
        "    display(Image('result.jpg'))\n",
        "\n",
        "def train(i):\n",
        "  loss = 0\n",
        "  img_out = image_f()\n",
        "  imgs_sliced = slice_imgs([img_out], samples, norm_in, uniform=uniform)\n",
        "  out_enc = perceptor.encode_image(imgs_sliced[-1])\n",
        "  loss = 0\n",
        "  if upload_image:\n",
        "    loss += -100*torch.cosine_similarity(img_enc, out_enc, dim=-1).mean()\n",
        "  if len(text) > 2:\n",
        "    loss += -100*torch.cosine_similarity(txt_enc, out_enc, dim=-1).mean()\n",
        "  if isinstance(loss, int): print(' Loss not defined, check the inputs'); exit(1)\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  \n",
        "  if i % save_freq == 0:\n",
        "    checkin(i // save_freq)\n",
        "\n",
        "outpic = ipy.Output()\n",
        "outpic\n",
        "\n",
        "pbar = ProgressBar(steps)\n",
        "for i in range(steps):\n",
        "  train(i)\n",
        "  _ = pbar.upd()\n",
        "\n",
        "HTML(makevid(tempdir))\n",
        "files.download('_out/ttt.mp4')\n",
        "if audio_notification == True: output.eval_js('new Audio(\"https://freesound.org/data/previews/80/80921_1022651-lq.ogg\").play()')\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}